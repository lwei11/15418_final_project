# Final Report

## Summary
We implemented data and pipeline model parallelism for a neural network that we built from scratch. We used MPI and tested both forms of parallelism on GHC. We varied different parameters such as batch size and number of hidden units to see how these affect the performance of each type of parallelism.

## Background
For our project, we implemented a deep neural network entirely from scratch (meaning no preexisting libraries or functions like Pytorch). Our network was made of a sequence of various types of layers such as linear layers, a sigmoid activation layer, and a softmax output layer. We used the linear and sigmoid layers for most of the calculations and the softmax layer to finalize the output. We wanted to test our parallelization in a model that used both linear and non-linear computations to make sure that we saw the benefits for different types of neural networks. 
### Key Data Structures
We have a few main data structures that we use for our deep neural network. The first is the weight matrices, which are used to actually train the model and keep track of the parameters for the computational layers. We frequently update these layers since they need to be changed every time the data is passed through one of our layers. Another data structure we use a lot is the bias vector. This is used to introduce a little more freedom into the data and allow the neural network to be more accurate when testing it at the end. Finally, we store the activations for each layer during forward propagation so we can use them again during the backward step. For training the network, we have some data and associated labels stored in a csv file which we import into our main file whenever we run the network. 
### Key Operations on Data Structures
When working with these data structures, the main tasks include matrix multiplications, element-wise operations (like applying the same function to each value), and vector transformations. Forward propagation is where the input data gets multiplied by weight matrices, bias values are added, and then a sigmoid activation function is applied to each result. Backpropagation is a bit more complex—it calculates how much the model’s predictions are off by using gradients. This involves transposing weight matrices and doing more matrix-vector multiplications. Since these calculations are repeated for every example or batch of data, they take a lot of computing power, which makes them perfect for parallel processing to save time.
### Algorithm Inputs and Outputs
The neural network starts by taking in feature vectors and at the other end, it outputs probabilities for each class (its predictions). During training, it also uses labels and calculates a loss function to measure how far off its guesses are. This loss function is important because it guides the adjustments to the weights and biases through gradient descent. At the end of the training process, the model has optimized weights and biases that can be used to predict new, unseen data.
### Why Parallelization Is Needed
The most time-consuming part of training a neural network is the matrix multiplications in forward and backward propagation. These calculations can really slow things down, especially with large networks or high-dimensional data. The runtime grows based on the number of neurons in each layer and how much data there is. That’s why parallelizing these operations is so useful—it cuts down on training time. There are a few ways to implement this, some of which we will showcase later.
### Workload Breakdown and Dependencies
The workload in a neural network naturally follows the sequence of layers. In forward propagation, each layer relies on the output of the layer before it. The same thing happens in backpropagation, but in reverse: each layer’s gradients depend on the ones calculated in the layer after it. However, between layers the calculations do not affect each other. This allows us to perform pipeline model parallelism since we can move data along the pipeline in parallel. Similarly, data parallelism works because gradients for different batches of data can be calculated at the same time. There is a bit of a challenge that comes when it’s time to sync everything up and make sure the gradients are properly combined.
### Parallelism and Locality
Neural networks offer plenty of opportunities for parallelism. Data parallelism splits the input data across multiple processes, which makes it very scalable and great for handling large datasets. Model parallelism is a bit more complicated to set up but works well for large networks that don’t fit into one process’s memory. The calculations themselves, like matrix multiplications and element-wise operations, would also work well with SIMD (Single Instruction, Multiple Data), where the same operation can be applied to many numbers at once.

## Approach
### Overview
We created two new training functions for our deep neural network that each utilized a different type of parallelization technique. The first used data parallelism where we split the data into batches and trained each batch in parallel. After every processor had trained on its batch and updated its weights, we aggregated them to get the final model. The other technique was pipeline model parallelism. We split the model so that every processor had its own layer to compute. The data was then split into micro batches and moved across the pipeline where each processor would start on the next batch of data as soon as it finished its previous one.
### Technologies Used
Our implementation was written in C++ and we used MPI for all parallelization/communication. The computers we targeted were multi core CPU processors where we attempted to use MPI to distribute work across them and communication information between them. For the technologies used for the neural network itself, it was completely implemented from scratch and did not use any preexisting libraries for setup, computation, or analysis. This allowed us to have a greater degree of control over what was parallelized and made sure no auto optimizations were implemented without our knowledge.
### Mapping the Problem to Parallel Machines
We utilized two main forms of parallelism: data and pipeline model parallelism. For our data parallel training function, we partitioned the data into chunks and assigned each to its own processor. At the end of the epoch or batch round, all gradients were aggregated using MPI_Allreduce and synched across all processors. This allowed for consistent models and better accuracy. See the diagram below for a visual of how this worked:
### DIAGRAM
For our pipeline model parallel training function, we divided the layers of a neural network and assigned them each to a processor. Because this required the division of functions, we had to hardcode it for different numbers of processors rather than dynamically allocate data as in the data parallel version. For this version of parallelism, the data was also split into chunks and these chunks were passed along the processors in a pipeline format. Each processor would receive a chunk, perform its assigned layer’s calculations on the chunk, and then pass it forward through the pipeline. At the end, each processor would then send the chunk backwards through the pipeline for the backpropagation step. Finally, the weights would be updated at the end. See the diagram below for the visualization of this version:
### DIAGRAM
### Changes to the Serial Algorithm
The main changes we made to the serial algorithm were batching and communication. Each form of parallelism used batching in some capacity and we had to change the algorithm to account for this. Additionally, for pipeline parallelism we had to change the forward and backward functions since we were splitting up the layers of the network. However, we did not parallelize the computations themselves i.e. the actual layers were left unchanged.
### Optimization Process
We tried a few different approaches to optimize both forms of parallelism. For data, we initially tried our own ring structure to pass messages around, but this ended up being fairly inefficient since we were passing weight matrices. There would be a lot of large messages passed, and there wasn’t a great opportunity to perform work while these were being passed since each processor needed updated gradients before working on the next batch or the accuracy would suffer a lot. So instead we ended up using MPI_Allreduce since we wanted to sum of the gradients to divide by the number of processors. This worked better overall. For pipeline model parallelism, we initially didn’t have a pipeline and just used model parallelism. But since we didn’t have an enormous neural network, this did not provide a whole lot of speedup, so we introduced the pipeline to make it better. We first tried doing it with smaller numbers of processors and asynchronous communication, but similar to before there wasn’t really a lot of time in between messages to do computation since each process needed the results from the previous layer to start. So instead we decided to use synchronous sends and receives to pass the parameter vectors through the network.
### Existing Code Base
The implementation was developed from scratch, with no reliance on external neural network libraries. This allowed us to maintain full control over the architecture and parallelization strategies, tailoring them to our specific use case. Our new training functions were designed specifically to integrate MPI operations into the training process, ensuring efficient use of parallel hardware.

## Results
Performance Measure and Experimental Speedup: We measure performance using speedup. Specifically, we compare the time of each run to the sequential program runtime on CPU for 200 or 400  hidden units per layer, 8 layers, batch size of 80 for mini-batch stochastic gradient descent, and 20 epochs, unless otherwise specified. We ran the experiments on GHC machines with this command: mpirun -np <number of processors> ./neuralnet inputs/small/small_train.csv inputs/small/small_validation.csv inputs/small/small_train_out.labels inputs/small/small_validation_out.labels inputs/small/small_metrics_out.txt <number of epochs> <number of hidden units> <init flag> <learning rate> <batch size>. 
